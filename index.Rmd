---
title: "Analyzing Text Data"
author: "Furqan Shah"
date: "2024-10-09"
output: github_document
---


# Introduction

In this tutorial, we will apply some basic textual analysis on data extracted from 23 companies' annual reports (provided on the repository for this page). The example data used is self-annotated, where narratives are classified into the following categories for each company annual report: Financial, Human, Intellectual, Natural, Social & Relationship, Manufactured, and an Unclassified category. This classification helps us explore different aspects of the companies' reports and analyze how they communicate various types of information. I will go through basic textual analysis tasks including cleaning, tokenizing, analyzing, and visualizing text data.

## Step 1: Load and Install Required Libraries

Before proceeding, we need to ensure that the necessary packages are installed.

```{r install-libraries, eval=TRUE, message=FALSE, warning=FALSE, include=TRUE}
# Function to install a package if not already installed
install_if_missing <- function(pkg) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, dependencies = TRUE)
    library(pkg, character.only = TRUE)
  }
}

# Load required packages and install if necessary
packages <- c("dplyr", "tidyr", "stringr", "tidytext", "ggplot2", "reshape2", "wordcloud2", "textdata", "wordcloud")

invisible(lapply(packages, install_if_missing))

```

## Step 2: Prepare and Load the Data

The data consists of 23 separate `.tsv` files, each representing a different company. Each file contains two columns: `Text` (which holds sentences or paragraphs) and `Theme` (the category assigned to the text). The categories in `Theme` column includee: **Financial**, **Human**, **Intellectual**, **Natural**, **Social & Relationship**, and **Unclassified**, as defined below:


- **Financial**: *Information about the company's financial performance, including revenue, expenses, profits, and financial health.*
- **Human**: *Relates to employees, their well-being, skills, and workforce management.*
- **Intellectual**: *Covers intangible assets like patents, trademarks, and the company’s innovation capacity.*
- **Natural**: *Focuses on environmental sustainability, resource usage, and the company’s ecological impact.*
- **Social & Relationship**: *Refers to the company’s interactions with stakeholders, including communities, customers, and regulators.*
- **Manufactured**: *Involves physical assets such as infrastructure, equipment, and operational capacity.*
- **Unclassified**: *Includes disclosures that do not fit into the other categories.*


```{r load-data, include=TRUE}
# List and read TSV files, add source column, combine, and remove missing rows/observations

# you need to replace path where to where you have stored the 23 .tsv files
path <- "F:/Github Projects/exploring-text-data/data files" 

data <- list.files(path = path, pattern = "\\.tsv$", full.names = TRUE) %>%
  lapply(function(file) read.delim(file) %>% mutate(source_file = basename(file))) %>%
  bind_rows() %>%
  rename(text = Text,
         theme = Theme,
         source = source_file) %>%
  mutate(across(everything(), ~na_if(str_trim(.), ""))) %>%
  drop_na()

glimpse(data)

data %>% count(source) # number of source files (should be 23)
data %>% count(theme) # number of categories assigned to text  

```

## Step 3: Clean and Prepare Text Data for Analysis

We will clean the text data in the `text` column by removing special characters, inserting spaces between words and numbers where needed, and eliminating any extra whitespace.

```{r clean-data, include=TRUE}
# Clean data
data2 <- data %>%
  mutate(text = str_replace_all(text, "[?âè.ã'’'½_&$,%'-']", " "), # Remove special characters
         text = str_replace_all(text, "([0-9])([a-z])", "\\1 \\2"),  # Add space between numbers and letters
         text = str_replace_all(text, "([a-z])([0-9])", "\\1 \\2"),  # Add space between letters and numbers
         text = str_squish(text))                                   # Strip extra white space
                                   
glimpse(data2)
```

Now we prepare the data for analysis by tokenizing the text. This means each sentence or paragraph in the `text` column is broken down into individual words, with each word becoming its own row. The following code performs this tokenization:

```{r tidy-data, include=TRUE}
# Tidy data - Unnest tokens
tidy_data <- data2 %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%                 # remove stop words          
  mutate(characters = nchar(word)) %>%      # number of alphabets in a word
  filter(characters > 1)                # filtering for words with > 1 alphabets


glimpse(tidy_data)
```
Notice the difference between the outputs of glimpse(data2) and glimpse(tidy_data). The previous code tokenized the text column, transforming each observation into individual words, allowing us to perform further analysis on a word-by-word basis.


## Step 4: Analyze and Visualize Text Data

### Top 15 Words

Next, we count the frequency of words used in each narrative reporting category. Then, we select the top 15 most frequent words for each category.

```{r top15-data, fig.width=10, fig.height=11, include=TRUE}
# Top 15 words 
top15_tidy <- tidy_data %>%
  filter(!str_detect(word, "^[0-9]*$")) %>%           # Exclude words that are just numbers
  count(word, theme) %>%
  group_by(theme) %>%
  slice_max(n, n = 15, with_ties = FALSE) %>%         # Select exactly top 10 words per theme
  ungroup() %>%
  arrange(theme, desc(n), word)                       # Arrange by theme, frequency, and word

glimpse(top15_tidy)
```

Now we use `ggplot` to visualize the top 15 most frequent words for each reporting category.

```{r top15.2-data, fig.width=10, fig.height=11, include=TRUE}
# Visualize top 15 words for each category
top15_tidy_plot <- top15_tidy %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col () +
  labs(x = NULL, y = "n") +
  facet_wrap(~theme, ncol = 2, scales = "free") +
  coord_flip()
top15_tidy_plot
```

We can see that the top 15 words for each category align with expectations. For example, words like *energy*, *emissions*, *carbon*, *environmental*, and *greenhouse* appear frequently in the **Natural** category, reflecting the focus on environmental topics.


### Word Cloud for "Natural" Category

One may also create a word cloud for a specific reporting category. Below, we generate a word cloud for the **Natural** reporting category.

```{r wordcloud-data, fig.width=10, fig.height=7, include=TRUE}
# World cloud - Natural 
theme_data <- tidy_data %>%
  filter(theme == "Natural") %>%              # You can replace "Natural" with any other category, e.g., "Human"
  filter(!str_detect(word, "^[0-9]*$")) %>%   # Remove words that are just numbers
  count(word) %>%
  arrange(desc(n))
  
wordcloud(words = theme_data$word, freq = theme_data$n, 
          scale = c(2, 0.5), random.order = FALSE, max.words = 200,
          colors = brewer.pal(8, "Dark2"))
```


### Word Cloud for "Natural" Category - Words Separated by Tone (Positive vs. Negative)

Below, we generate a word cloud for the **Natural** category, with words separated by tone (positive vs. negative).


```{r wordcloud.2-data, fig.width=10, fig.height=11, include=TRUE}
# Positive and Negative Tone Word Cloud
tidy_data %>%
  filter(theme == "Natural") %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(max.words = 200, title.size = 3, scale = c(3, 1.3))
```


## Conclusion

In this tutorial, I demonstrate how to conduct basic text data analysis using a dataset of 23 companies' annual reports, with narratives classified in different reporting themes. The focus was on cleaning, tokenizing, and visualizing the text. By following these steps, one can apply similar techniques to other textual datasets or any form of unstructured text data to extract insights.








